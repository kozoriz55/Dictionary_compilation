# –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç—ñ–≤ –Ω–æ–≤–∏–Ω Êñ∞Âçé –∑–∞–≥–æ–ª–æ–≤–∫–∏ + –ø–æ—Å–∏–ª–∞–Ω–Ω—è + –Ω–æ–≤–∏–Ω–∞, –≤–µ—Ä—Å—ñ—è ChatGpt
import requests
import json
from bs4 import BeautifulSoup
import time
import csv
import re

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Referer": "https://www.news.cn/",
    "X-Requested-With": "XMLHttpRequest"
}

def fetch_news_page(keyword, page=1):
    url = "https://so.news.cn/getNews"
    params = {
        "lang": "cn",
        "curPage": page,
        "searchFields": 1,
        "sortField": 0,
        "keyword": keyword
    }
    response = requests.get(url, headers=HEADERS, params=params)
    print(f"üì° –°—Ç–æ—Ä—ñ–Ω–∫–∞ {page} ‚Äî –°—Ç–∞—Ç—É—Å: {response.status_code}")
    
    if response.status_code == 200:
        try:
            return response.json()
        except json.JSONDecodeError:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –¥–µ–∫–æ–¥—É–≤–∞—Ç–∏ JSON.")
            print(response.text[:300])  # –≤–∏–≤–æ–¥–∏—Ç—å —á–∞—Å—Ç–∏–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            return None

    else:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ HTTP {response.status_code}")
        return None

def extract_article_text(url):
    try:
        r = requests.get(url, headers=HEADERS)
        r.encoding = 'utf-8'
        soup = BeautifulSoup(r.text, 'lxml')

        candidates = [
            ("div", {"class": "article"}),
            ("div", {"class": "mainCon", "id": "content"}),
            ("div", {"id": "detail"}),
            ("div", {"class": "main"}),
            ("div", {"id": "content"}),
            ("div", {"class": "content"}),
            ("div", {"class": "zw"}),
            ("div", {"class": "con_txt"}),
        ]

        for tag, attrs in candidates:
            container = soup.find(tag, attrs=attrs)
            if container:
                paragraphs = container.find_all("p")
                return ''.join(p.get_text(strip=True) for p in paragraphs)

        return soup.get_text(strip=True)
    except Exception as e:
        print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥–Ω—É—Ç–∏ —Ç–µ–∫—Å—Ç: {url} ‚Äî {e}")
        return "–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–æ"

def main():
    keyword = "‰πåÂÖãÂÖ∞"
    first_page = fetch_news_page(keyword, 1)
    if not first_page:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–µ—Ä—à—É —Å—Ç–æ—Ä—ñ–Ω–∫—É.")
        return

    total_pages = first_page.get("content", {}).get("pageCount", 1)
    seen_titles = set()

    with open("ukraine_news_xinhua.csv", "a", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["–ó–∞–≥–æ–ª–æ–≤–æ–∫", "URL", "–¢–µ–∫—Å—Ç"])

        for page in range(1, total_pages + 1):
            print(f"üîÑ –û–±—Ä–æ–±–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page}/{total_pages}... ")
            data = fetch_news_page(keyword, page)
            if not data or "content" not in data or "results" not in data["content"]:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É {page}: –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å.")
                continue

            for item in data.get("content", {}).get("results", []):
                try:
                    title = re.sub('<[^>]+>|&nbsp;', '', item.get("title", "")).strip()
                    url = item.get("url")
                    if title and url and title not in seen_titles:
                        text = extract_article_text(url)
                        writer.writerow([title, url, text])
                        seen_titles.add(title)
                        print(f"‚úÖ {title}")
                        time.sleep(1)  # –∑–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
                except Exception as e:
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –Ω–æ–≤–∏–Ω–∏: {e}")
                    continue

    print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É 'ukraine_news_xinhua.csv'")

if __name__ == "__main__":
    main()
